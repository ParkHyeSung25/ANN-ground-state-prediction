# -*- coding: utf-8 -*-
"""2021_8_13_deuteron_jacobi_frame.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YYsfiYDN0zBIrPzsuN14qWveRNbu-wjC

[!Open in Collab](https://colab.research.google.com/github/coreyjadams/AI4NP_School/blob/main/Hydrogen Atom.ipynb)

# Hydrogen Atom

In the previous notebook, we solved the harmonic oscillator for 1 dimension using artifical neural networks.  In this notebook, we'll go a step further and solve a more complicated system (Hydrogen) in more dimensions (3 of course).  We will use more sophisticated sampling of the wavefunction and compute the proper $\nabla^2$ operator.
"""

# Commented out IPython magic to ensure Python compatibility.
import tensorflow as tf
import numpy

# What numerical precision?
# Note: float32 == single precision is OK for this notebook.  Higher order methods need float64 for a matrix inversion step.
DEFAULT_TENSOR_TYPE = tf.float32

# How many walkers should we use?
N_WALKERS = 500
# How many times should we make an observation of the observables each iteration?
N_OBSERVATIONS = 30

from matplotlib import pyplot as plt
from mpl_toolkits import mplot3d
# %matplotlib inline

import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
from IPython.display import display, clear_output

"""## Surrogate Wavefunction

For the hydrogen atom, we'll use a simple surrogate wavefunction.

NOTE that the wavefunction is a surrogate for $log \psi$, so the multiplicative initial boundary condition is a sum.
"""

import numpy
import tensorflow as tf


class NeuralWavefunction(tf.keras.models.Model):
    """Create a neural network eave function in N dimensions

    Boundary condition, if not supplied, is gaussian in every dimension

    Extends:
        tf.keras.models.Model
    """
    def __init__(self, ndim : int, initial_containment : float = -0.1):
        tf.keras.models.Model.__init__(self)

        self.ndim = ndim


        self.layer1 = tf.keras.layers.Dense(24, use_bias=False)
        self.layer2 = tf.keras.layers.Dense(32, use_bias=False)
        self.layer3 = tf.keras.layers.Dense(1, use_bias=False)

        self.containment = tf.Variable(initial_containment, trainable = True)

    @tf.function
    def call(self, inputs):
        #
        # shape is [nwalkers, dim]

        x = self.layer1(inputs)
        x = tf.keras.activations.softplus(x)
        # WHat is softplus?  It's like ReLU at large absolute value, but is smooth through 0.0

        x = self.layer2(x)
        x = tf.keras.activations.softplus(x)

        # Just one output:
        x = self.layer3(x)
        # x = tf.keras.activations.tanh(x)

        # Compute the initial boundary condition, which the network will slowly overcome
        # This is important because it keeps the walkers bounded, otherwise they will not converge to a bound state.
        r = tf.sqrt(tf.reduce_sum(inputs**2, axis=1))
        # We use the exponential to map the countainment to positive-only values while having a smooth gradient
        boundary_condition = -tf.exp(self.containment) * r
        boundary_condition = tf.reshape(boundary_condition, [-1,1])


        return x + boundary_condition

ground_state = NeuralWavefunction(3, initial_containment = 0.5)

"""## Metropolis Sampling

To sample the wave function, we'll use the metropolis algorithm.  We can implement this algorithm in tensorflow as shown below:
"""

class MetropolisSampler(object):
    """Metropolis Sampler in N dimension

    Sample from N-D coordinates, using some initial probability distribution

    Relies on functional calls to sample on the fly with flexible distributions
    """
    def __init__(self,
        n           : int,
        nwalkers    : int,
        initializer : callable,
        init_params : iter ,
        dtype       = tf.float64):
        '''Initialize a metropolis sampler

        Create a metropolis walker with `n` walkers.  Can use normal, uniform

        Arguments:
            n {int} -- Dimension
            nwalkers {int} -- Number of unique walkers
            initializer {callable} -- Function to call to initialize each walker
            init_params {iter} -- Parameters to pass to the initializer, unrolled automatically
        '''

        # Set the dimension:
        self.n = n

        # Set the number of walkers:
        self.nwalkers = nwalkers

        self.size = (self.nwalkers, self.n)

        self.dtype = dtype

        #  Run the initalize to get the first locations:
        self.walkers = initializer(shape=self.size, **init_params, dtype=dtype)

    def sample(self):
        '''Just return the current locations

        '''
        # Make sure to wrap in tf.Variable for back prop calculations
        return  self.walkers

    def kick(self,
        wavefunction : tf.keras.models.Model,
        kicker : callable,
        kicker_params : iter,
        nkicks : int ):
        '''Wrapper for a compiled kick function via tensorflow.

        This fills in the compiled function with the size and the walkers.

        Arguments:
            wavefunction {tf.keras.models.Model} -- The wavefunction used for the metropolis walk
            kicker {callable} -- A callable function for generating kicks
            kicker_params {iter} -- Arguments to the kicker function.
        '''
        # for i in range(nkicks):
        walkers, acceptance = self.internal_kicker(
            self.size, self.walkers, wavefunction, kicker, kicker_params, tf.constant(nkicks), dtype=self.dtype)

        # Update the walkers:
        self.walkers = walkers

        # Send back the acceptance:
        return acceptance

    @tf.function
    def internal_kicker(self,
        _shape,
        _walkers,
        _wavefunction : tf.keras.models.Model,
        _kicker : callable,
        _kicker_params : iter,
        _nkicks : tf.constant,
        dtype):
        """Sample points in N-d Space

        By default, samples points uniformly across all dimensions.
        Returns a torch tensor on the chosen device with gradients enabled.

        Keyword Arguments:
            kicker {callable} -- Function to call to create a kick for each walker
            kicker_params {iter} -- Parameters to pass to the kicker, unrolled automatically
        """


        # We need to compute the wave function twice:
        # Once for the original coordiate, and again for the kicked coordinates
        acceptance = tf.convert_to_tensor(0.0, dtype=dtype)
        # Calculate the current wavefunction value:
        current_wavefunction = _wavefunction(_walkers)

        # Generate a long set of random number from which we will pull:
        random_numbers = tf.math.log(tf.random.uniform(shape = [_nkicks,_shape[0],1], dtype=dtype))

        # Generate a long list of kicks:
        # print(shape)
        kicks = _kicker(shape=[_nkicks, *_shape], **_kicker_params, dtype=dtype)
        # print(kicks.shape)

        for i_kick in tf.range(_nkicks):
            # Create a kick:
            kick = kicks[i_kick]
            # kick = kicker(shape=shape, **kicker_params, dtype=dtype)
            kicked = _walkers + kick

            # Compute the values of the wave function, which should be of shape
            # [nwalkers, 1]
            kicked_wavefunction   = _wavefunction(kicked)


            # Probability is the ratio of kicked **2 to original
            probability = 2 * (kicked_wavefunction - current_wavefunction)
            # Acceptance is whether the probability for that walker is greater than
            # a random number between [0, 1).
            # Pull the random numbers and create a boolean array
            # accept      = probability >  tf.random.uniform(shape=[shape[0],1])
            accept      = probability >  random_numbers[i_kick]
            # accept      = probability >  tf.math.log(tf.random.uniform(shape=[shape[0],1]))

            # Grab the kicked wavefunction in the places it is new, to speed up metropolis:
            current_wavefunction = tf.where(accept, kicked_wavefunction, current_wavefunction)

            # We need to broadcast accept to match the right shape
            # Needs to come out to the shape [nwalkers, nparticles, ndim]
            accept = tf.tile(accept, [1,tf.reduce_prod(_shape[1:])])
            accept = tf.reshape(accept, _shape)
            _walkers = tf.where(accept, kicked, _walkers)

            acceptance = tf.reduce_mean(tf.cast(accept, dtype))

        return _walkers, acceptance

"""Now that we have a surrogate model and a sampling tool, we can test things:"""

sampler = MetropolisSampler(
            n           = 3, # 3 dimensions
            nwalkers    = N_WALKERS,
            initializer = tf.random.normal,
            init_params = {"mean": 0.0, "stddev" : 0.2},
            dtype       = DEFAULT_TENSOR_TYPE
    )

ground_x = sampler.sample()

"""If we look at X, we should see a 100 x 3 tensor.  Elements from the tensor should roughly follow a gaussian distribution since that's what we used to initialize it (mean 0, STD 0.2)"""

print(ground_x.shape)
print(tf.reduce_mean(ground_x))
print(tf.math.reduce_std(ground_x))

"""Now we can print a summary of the network, too:"""

values = ground_state(ground_x)
ground_state.summary()
print(values.shape)

"""Note: the sum of all printed parameters is one short than the total number of trainable parameters.  That's because the confinement is also a trainable parameter!

## Thermalize the walkers

To thermalize the walkers, we can "kick" them a number of times according to the metropolis algorithm:
"""

r_before_thermalization = tf.sqrt(tf.reduce_sum(sampler.sample()**2, axis=1)).numpy()
acceptance = sampler.kick(
    wavefunction  = ground_state ,
    kicker        = tf.random.normal,
    kicker_params = {"mean": 0.0, "stddev" : 0.3},
    nkicks        = 5000 )
# This gets the latest walkers
ground_x = sampler.sample()
print("Acceptance: ", acceptance)
r_after_thermalization = tf.sqrt(tf.reduce_sum(sampler.sample()**2, axis=1)).numpy()

print(tf.reduce_mean(ground_x))
print(tf.math.reduce_std(ground_x))

bins = numpy.arange(0,10,0.5)

print(f"Mean r before thermalization: {tf.reduce_mean(r_before_thermalization):.4f}")
print(f"Mean r after thermalization: {tf.reduce_mean(r_after_thermalization):.4f}")

before_hist, edges = numpy.histogram(r_before_thermalization, bins=bins)
after_hist,  edges = numpy.histogram(r_after_thermalization, bins=bins)

bin_centers = 0.5*(edges[1:] + edges[:-1])


fig = plt.figure(figsize=(16,9))
plt.plot(bin_centers, before_hist, marker="+", label="Before Therm.")
plt.plot(bin_centers, after_hist, marker="o", label="After Therm.")
plt.grid(True)
plt.legend()
plt.show()

print(acceptance)

"""## Hydrogen Atom Hamiltonian

Here's an implementation of the hydrogen atom that computes observables:
"""

import tensorflow as tf
import numpy
import math

import logging
logger = logging.getLogger()
DEFAULT_TENSOR_TYPE2 = tf.float64


class HydrogenAtom(object):

    def __init__(self):

        object.__init__(self)
        self.HBAR = tf.constant(197.3269804, dtype = DEFAULT_TENSOR_TYPE2)


    @tf.function
    def compute_derivatives(self, wavefunction : tf.keras.models.Model, inputs : tf.Tensor):


        # Turning off all tape watching except for the inputs:
        # Using the outer-most tape to watch the computation of the first derivative:
        with tf.GradientTape(persistent=True) as tape:
            # Use the inner tape to watch the computation of the wavefunction:
            tape.watch(inputs)
            with tf.GradientTape() as second_tape:
                second_tape.watch(inputs)
                logw_of_x = wavefunction(inputs, training=True)
            # Get the derivative of logw_of_x with respect to inputs
            dlogw_dx = second_tape.gradient(logw_of_x, inputs)

        # Get the derivative of dlogw_dx with respect to inputs (aka second derivative)

        # We have to extract the diagonal of the jacobian, which comes out with shape
        # [nwalkers, dimension, dimension]

        # This is the full hessian computation:
        d2logw_dx2 = tape.batch_jacobian(dlogw_dx, inputs)


        # And this contracts:
        d2logw_dx2 = tf.einsum("wdd->wd",d2logw_dx2)

        return logw_of_x, dlogw_dx, d2logw_dx2


    @tf.function
    def energy(self, wavefunction : tf.keras.models.Model, inputs : tf.Tensor):

        inputs = tf.cast(inputs, DEFAULT_TENSOR_TYPE2)
        logw_of_x, dlogw_dx, d2logw_dx2 = self.compute_derivatives(wavefunction, inputs)

        interaction, ke_direct = self.compute_energies(inputs, logw_of_x, dlogw_dx, d2logw_dx2)

        energy = tf.squeeze(interaction + ke_direct)
        #energy_jf = tf.squeeze(pe+ke_jf)

        return tf.cast(energy, tf.float32), tf.cast(ke_direct, tf.float32), tf.cast(interaction, tf.float32)

    @tf.function
    def interaction(self, *, inputs):
    #     Gamma = tf.constant(-4.0, dtype = DEFAULT_TENSOR_TYPE2)
    #     D0 = tf.constant(0.677*1000.0, dtype = DEFAULT_TENSOR_TYPE2)
    #     C1 = tf.constant(-0.505*1000.0, dtype = DEFAULT_TENSOR_TYPE2)
    #     C2 = tf.constant(-0.435*1000.0, dtype = DEFAULT_TENSOR_TYPE2)
        r = tf.reduce_sum(inputs**2, axis=1)
        # interaction = (tf.constant(1.0/4.0,dtype = DEFAULT_TENSOR_TYPE2))*((tf.constant(3.0,dtype = DEFAULT_TENSOR_TYPE2) * C1) + C2 + (C1 - C2))*tf.exp(Gamma*r) #-교체

        interaction = 1.0/4.0*(3.0 * (-0.142*1000.0) -0.106*1000.0 + (-0.142 + 0.106)*1000.0)*tf.exp(-1.0*r)
        return interaction

    @tf.function
    def tot_kinetic_energy(self, *, logw_of_x: tf.Tensor, dlogw_dx: tf.Tensor, d2logw_dx2 : tf.Tensor):
        mass1 = tf.constant(939.565420, dtype = DEFAULT_TENSOR_TYPE2)
        mass2 = tf.constant(938.272029, dtype = DEFAULT_TENSOR_TYPE2)
        M1M2= mass1*mass2
        sum_m = mass1 + mass2
        Mass = (M1M2/sum_m)

        ke_i = -(self.HBAR**2 / (2 * tf.cast(Mass, DEFAULT_TENSOR_TYPE2))) * (tf.reduce_sum(d2logw_dx2, axis=(1)) + tf.reduce_sum(dlogw_dx**2, axis=(1)))
        ke = ke_i

        return tf.cast(ke, dtype= DEFAULT_TENSOR_TYPE2)



    @tf.function
    def compute_energies(self, inputs, logw_of_x, dlogw_dx, d2logw_dx2):

        interaction = self.interaction(inputs=inputs)

        ke_direct = self.tot_kinetic_energy(logw_of_x = logw_of_x,dlogw_dx=dlogw_dx, d2logw_dx2 = d2logw_dx2)

        return interaction, ke_direct

hamiltonian = HydrogenAtom()

energy, ke_direct, interaction = hamiltonian.energy(ground_state, ground_x)

print(tf.reduce_mean(energy))

"""In the units we're using (atomic units) the value of the hydrogen ground state comes out to -0.5.  So, this is good: the energy is above the ground state for our random wavefunction which is good!

## Optimizing the wavefunction

To optimize the wavefunction, we'll need to compute several observables based on the jacobian.  Here are some functions to help contained within a `compute_gradients` function.
"""

def compute_gradients(N_OBSERVATIONS, _sampler, _wavefunction, _hamiltonian):

    # Helper function to compute jacobians:
    @tf.function
    def jacobian(j_x_current, j_wavefunction):
        tape = tf.GradientTape()

        with tape:
            tape.watch(j_wavefunction.trainable_variables)
            log_wpsi = j_wavefunction(j_x_current)

    #     print(log_wpsi)


        jac = tape.jacobian(log_wpsi, j_wavefunction.trainable_variables)

    #     print(wavefunction.trainable_variables)
    #     print(jac)

        # Grab the original shapes ([1:] means everything except first dim):
        jac_shape = [j.shape[1:] for j in jac]
        # get the flattened shapes:
        flat_shape = [[-1, tf.reduce_prod(js)] for js in jac_shape]
        # Reshape the

        # We have the flat shapes and now we need to make the jacobian into a single matrix

        flattened_jacobian = [tf.reshape(j, f) for j, f in zip(jac, flat_shape)]

        flattened_jacobian = tf.concat(flattened_jacobian, axis=-1)

        return flattened_jacobian, flat_shape


    # Helper function to compute observables:
    @tf.function
    def compute_O_observables(flattened_jacobian, energy):

        # dspi_i is the reduction of the jacobian over all walkers.
        # In other words, it's the mean gradient of the parameters with respect to inputs.
        # This is effectively the measurement of O^i in the paper.
        dpsi_i = tf.reduce_mean(flattened_jacobian, axis=0)
        dpsi_i = tf.reshape(dpsi_i, [-1,1])

        # To compute <O^m O^n>
        dpsi_ij = tf.linalg.matmul(flattened_jacobian, flattened_jacobian, transpose_a = True) / N_WALKERS

        # Computing <O^m H>:
        dpsi_i_EL = tf.linalg.matmul(tf.reshape(energy, [1,N_WALKERS]), flattened_jacobian)
        # This makes this the same shape as the other tensors
        dpsi_i_EL = tf.reshape(dpsi_i_EL, [-1, 1])

        return dpsi_i, dpsi_i_EL, dpsi_ij


#     @tf.function(experimental_relax_shapes=True)
    def compute_SR_gradients(crs_energy, csr_dpsi_i, csr_dpsi_i_EL, csr_dpsi_ij):

        eps = 0.001
        npt = csr_dpsi_i.shape[0]

        # These are the first order gradients:
        gradients = tf.cast(( csr_dpsi_i * crs_energy - csr_dpsi_i_EL ), tf.float64)

        # For the matrix inversion, we want to use higher precision:
        S_ij = tf.cast(csr_dpsi_ij - csr_dpsi_i * tf.transpose(csr_dpsi_i), tf.float64)

        # We loop until the inversion comes out positive definite:
        i = 0
        while True:
            S_ij_d = S_ij + 2**i * eps * tf.eye(npt, dtype=tf.float64)
            i += 1
            try:
                U_ij = tf.linalg.cholesky(S_ij_d)
                positive_definite = True
            except:
                positive_definite = False
                print(f"Cholesky solve did not find a positive definite matrix on attempt {i}!")

            if positive_definite:
                gradients = tf.linalg.cholesky_solve(U_ij, gradients)

                break

        return tf.cast(gradients, tf.float32)

    # Places to accumulate observables:
    dpsi_i    = None
    dpsi_i_EL = None
    dpsi_ij   = None
    obs_energy = 0

    for i_obs in range(N_OBSERVATIONS):

        # First, we kick the sampler to re-thermalize to the new wavefunction:
        kicker = tf.random.normal
        kicker_params = {"mean": 0.0, "stddev" : 0.4}

        acceptance = _sampler.kick(_wavefunction, kicker, kicker_params, nkicks=100)


        # Get the current walker locations:
        x_current  = _sampler.sample()

        # Compute the observables:
        energy, ke_direct, interaction = hamiltonian.energy(_wavefunction, x_current)

        energy /= N_WALKERS


        # For each observation, we compute the jacobian.
        # flattened_jacobian is a list, flat_shape is just one instance
        flattened_jacobian, flat_shape = jacobian(x_current, _wavefunction)

        _dpsi_i, _dpsi_i_EL, _dpsi_ij = compute_O_observables(flattened_jacobian, energy)
        if dpsi_i is None:
            dpsi_i = _dpsi_i
        else:
            dpsi_i += _dpsi_i

        if dpsi_i_EL is None:
            dpsi_i_EL = _dpsi_i_EL
        else:
            dpsi_i_EL += _dpsi_i_EL

        if dpsi_ij is None:
            dpsi_ij = _dpsi_ij
        else:
            dpsi_ij += _dpsi_ij

        obs_energy += tf.reduce_sum(energy)

    obs_energy /= N_OBSERVATIONS
    dpsi_i     /= N_OBSERVATIONS
    dpsi_i_EL  /= N_OBSERVATIONS
    dpsi_ij    /= N_OBSERVATIONS


    gradients = compute_SR_gradients(obs_energy, dpsi_i, dpsi_i_EL, dpsi_ij)


    # Lastly, reshape the gradients to match the weights:
    running_index = 0
    gradient = []
    for length in flat_shape:
        l = length[-1]
        end_index = running_index + l
        gradient.append(gradients[running_index:end_index])
        running_index += l

    shapes = [ p.shape for p in _wavefunction.trainable_variables ]
    gradients = [ tf.reshape(g, s) for g, s in zip(gradient, shapes)]
    return gradients, obs_energy

# optimizer = tf.keras.optimizers.Adam()
energy_history = []
steps_history  = []
LEARNING_RATE  = 0.001

"""## Optimization Loop"""

# Define a figure here which we can update along the way:
fig       = plt.figure(figsize=(16,9))
# Create 2x2 sub plots
gs = gridspec.GridSpec(2, 2)
ax_loss   = plt.subplot(gs[:,0])
ax_denst  = plt.subplot(gs[:,1], projection="3d")



for i in range(600):

    gradients, energy = compute_gradients(N_OBSERVATIONS, sampler, ground_state, hamiltonian)

    # Scale by the learning rate
    gradients = [LEARNING_RATE * g for g in gradients ]

    for i_param in range(len(ground_state.trainable_variables)):
        ground_state.trainable_variables[i_param].assign_add(gradients[i_param])
    print(tf.reduce_mean(energy))

    energy_history.append(energy)
    steps_history.append(i)

    # Here we update the plots.  Show the energy function:
    ax_loss.set_xlim(0, 1.1*i)
    ax_loss.cla()
    ax_loss.plot(steps_history, energy_history, label="Energy")
    ax_loss.grid(True)
    ax_loss.legend(fontsize=25)

    # Show the location of the walkers:
    x, y, z = tf.split(sampler.sample(), 3, axis=1)
    ax_denst.clear()
    ax_denst.set_xlim(-4,4); ax_denst.set_ylim(-4,4); ax_denst.set_zlim(-4,4);
    ax_denst.set_xlabel("X"); ax_denst.set_ylabel("Y"); ax_denst.set_zlabel("Z");
    ax_denst.scatter(x, y, z, linewidth=0.5);

    display(fig)
    clear_output(wait = True)
#     plt.pause(0.25)



# Want to save your model?  Use this syntax
# tf.keras.models.save_model(ground_state, "HydrogenGroundState")

"""This can take a while to converge.  If you are running on, say, a CPU or just don't want to wait, you can access my pretrained model:"""

# Run this if you cloned the repo locally:
# ground_state = tf.keras.models.load_model("saved_models/HydrogenGroundState")

# Run this if you are on colab:
# !git clone https://github.com/coreyjadams/AI4NP_School.git
# ground_state = tf.keras.models.load_model("AI4NP_School/saved_models/HydrogenGroundState")

"""This checks the final energy computed:"""

kicker = tf.random.normal
kicker_params = {"mean": 0.0, "stddev" : 0.4}

acceptance = sampler.kick(ground_state, kicker, kicker_params, nkicks=2000)
x = sampler.sample()
energy, ke_direct, interaction = hamiltonian.energy(ground_state, x)
print(tf.reduce_mean(energy))

print(ground_state.containment) # (If you load my model, this should be 0.011789099)

print(ground_state.containment) # (If you load my model, this should be 0.011789099)

import numpy
# energy_history[30]=
average_array = []
average = 0
for i in range(len(energy_history)-int(len(energy_history)/10),len(energy_history)):
  average = average + energy_history[i]
  average_array.append(energy_history[i])

average = average / int(len(energy_history)/10)
std = numpy.std(average_array)
print(average, std)

"""### Where are the walkers?

Now that we've trained the wavefunction, where are the walkers?
"""

r_ground  = tf.sqrt(tf.reduce_sum(ground_x**2, axis=1))

print(f"Mean r for the ground state: {tf.reduce_mean(r_ground):.4f}")

bins = numpy.arange(0,15,0.5)

ground_hist,  edges = numpy.histogram(r_ground, bins=bins)

bin_centers = 0.5*(edges[1:] + edges[:-1])


fig = plt.figure(figsize=(16,9))
plt.scatter(bin_centers, ground_hist, marker="+", label="Ground State")
plt.grid(True)
plt.legend()
plt.show()

"""## Computing Observables

If you look online ([here](http://farside.ph.utexas.edu/teaching/qmech/Quantum/node82.html) for example) there are many resources that tell you about the hydrogen atom's observable values.  $<r>$ for example should be equal to $\frac{a_0}{2} [ 3 n^2 - l(l+1)]$.  We don't have any angular momentum in the ground state, and the value of $a_0$ in our units is 1.0.  So, $<r>$ should be 1.5
"""

r_accum = 0.0
r2_accum = 0.0

kicker = tf.random.normal
kicker_params = {"mean": 0.0, "stddev" : 0.4}

for i in range(N_OBSERVATIONS):
    acceptance = sampler.kick(ground_state, kicker, kicker_params, nkicks=200)
    ground_x = sampler.sample()

    r2 = tf.reduce_sum(ground_x**2, axis=1)
    r2_accum += tf.reduce_sum(r2)
    r_ground  = tf.sqrt(r2)
    r_accum  += tf.reduce_sum(r_ground)


n_walkers = ground_x.shape[0]
r_accum /= N_OBSERVATIONS*n_walkers
r_err = 1/(N_OBSERVATIONS*n_walkers - 1) * tf.sqrt(r2_accum - r_accum**2)
print(f"Expectation of radius is: {r_accum:.3f} +/ {r_err:.3f}")

"""On the other side, $<1/r>$ should be equal to 1.0 in our units:"""

r_accum = 0.0
r2_accum = 0.0

kicker = tf.random.normal
kicker_params = {"mean": 0.0, "stddev" : 0.4}


for i in range(N_OBSERVATIONS):
    acceptance = sampler.kick(ground_state, kicker, kicker_params, nkicks=200)
    ground_x = sampler.sample()

    r2 = 1. / tf.reduce_sum(ground_x**2, axis=1)
    r2_accum += tf.reduce_sum(r2)

    r_ground  = tf.sqrt(r2)
    r_accum += tf.reduce_sum(r_ground)

n_walkers = ground_x.shape[0]
r_accum /= N_OBSERVATIONS*n_walkers
r_err = 1/(N_OBSERVATIONS*n_walkers - 1) * tf.sqrt(r2_accum - r_accum**2)

print(f"Expectation of 1/radius is: {r_accum:.3f} +/- {r_err:.3f}")

"""## Angular momentum

The angular momentum operator $L_z$ has good quantum numbers, so let's compute it's expectation value for the ground state.  Remember, here, we parametrize the log of the wavefunction ($log(\psi)$).  Note that:

### $\frac{\partial}{\partial x} log(\psi) == \frac{1}{\psi} \frac{\partial \psi}{\partial x}$

and

### $L_z = -i \hbar (x \frac{\partial \psi}{\partial y} - y \frac{\partial \psi}{\partial x}) $

To compute the observable, we sum:

### $<L_z> = \sum \frac{L_z}{\psi}  = -i \hbar \sum \left( x \frac{1}{\psi} \frac{\partial \psi}{\partial y} -  y  \frac{1}{\psi} \frac{\partial \psi}{\partial x} \right)$

or, using the first equation above:

### $<L_z> = -i \hbar \sum \left( x \frac{\partial}{\partial y} log(\psi) - y \frac{\partial}{\partial x} log(\psi) \right) $

We can compute the z component of angular momentum, then, in units of $-i \hbar$:
"""

lz_accum = 0.0
lz2_accum = 0.0

l2_accum = 0.0
l22_accum = 0.0

kicker = tf.random.normal
kicker_params = {"mean": 0.0, "stddev" : 0.4}

for i in range(N_OBSERVATIONS):

    acceptance = sampler.kick(ground_state, kicker, kicker_params, nkicks=200)
    ground_x = sampler.sample()


    logw_of_x, dlogw_dx, d2logw_dx2 = hamiltonian.compute_derivatives(ground_state,ground_x)

#     print(ground_x.shape)

    lx = ground_x[:,1]*dlogw_dx[:,2] - ground_x[:,2] * dlogw_dx[:,1]
    ly = ground_x[:,2]*dlogw_dx[:,0] - ground_x[:,0] * dlogw_dx[:,2]
    lz = ground_x[:,0]*dlogw_dx[:,1] - ground_x[:,1] * dlogw_dx[:,0]

    lz_accum  += tf.reduce_sum(lz)
    lz2_accum += tf.reduce_sum(lz**2)


    l2 = tf.sqrt(lx**2 + ly**2 + lz**2)
    l2_accum += tf.reduce_sum(l2)
    l22_accum += tf.reduce_sum(l2**2)
n_walkers = ground_x.shape[0]

lz_accum /= N_OBSERVATIONS*n_walkers
lz_err = 1/(N_OBSERVATIONS*n_walkers - 1) * tf.sqrt(lz2_accum - lz_accum**2)

print(f"Expectation of L_z is: -i * hbar * {lz_accum:.4f} +/- {lz_err:.4f}")

l2_accum /= N_OBSERVATIONS*n_walkers
l2_err = 1/(N_OBSERVATIONS*n_walkers - 1) * tf.sqrt(l22_accum - l2_accum**2)

print(f"Expectation of L^2 is: -i * hbar * {l2_accum:.4f} +/- {l2_err:.4f}")

