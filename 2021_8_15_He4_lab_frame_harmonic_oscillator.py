# -*- coding: utf-8 -*-
"""2021_8_15_He4_lab_frame_harmonic_oscillator.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12iRr3GEWsvFKIt3SPQL4adYRDGAURhSj

# 3 body problem

In the previous notebook, we solved the harmonic oscillator for 1 dimension using artifical neural networks.  In this notebook, we'll go a step further and solve a more complicated system (Hydrogen) in more dimensions (3 of course).  We will use more sophisticated sampling of the wavefunction and compute the proper $\nabla^2$ operator.
"""

# Commented out IPython magic to ensure Python compatibility.
import tensorflow as tf
import numpy
import math
import logging

from matplotlib import pyplot as plt
from mpl_toolkits import mplot3d
# %matplotlib inline
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
import matplotlib.ticker as mticker
from IPython.display import display, clear_output
from sympy  import Integral, Symbol

logger = logging.getLogger()
DEFAULT_TENSOR_TYPE = tf.float32
DEFAULT_TENSOR_TYPE2 = tf.float64

# wave function puff constant
N_WALKERS = 500
N_OBSERVATIONS = 30

# natural unit
hbar = 1.0
c = 1.0
epsilon0 = 1.0
fm = 1.0
fm_inverse = 1/fm
hbarc = 1.0 #197.3269804 * Mev * fm 이므로 아래 값들 정의 가능
MeV = 1/197.327
GeV = 1000.0*MeV
alpha = 1./137.036

# nucleon mass
m = []
Nm = 939.565
Pm = 938.272
for N in range (2):
  m.append(Nm)
for P in range (2):
  m.append(Pm)

"""## Surrogate Wavefunction

For the hydrogen atom, we'll use a simple surrogate wavefunction.

NOTE that the wavefunction is a surrogate for $log \psi$, so the multiplicative initial boundary condition is a sum.
"""

import numpy
import tensorflow as tf


class NeuralWavefunction(tf.keras.models.Model):

    def __init__(self, ndim : int,
                 initial_containment1 : float = -0.1,
                 initial_containment2 : float = -0.1):

        tf.keras.models.Model.__init__(self)
        self.ndim = ndim

        self.layer1 = tf.keras.layers.Dense(16, use_bias=False)
        self.layer2 = tf.keras.layers.Dense(24, use_bias=False)
        self.layer3 = tf.keras.layers.Dense(1, use_bias=False)

        self.containment1 = tf.Variable(initial_containment1, trainable = True)
        self.containment2 = tf.Variable(initial_containment2, trainable = True)
        self.MeV = MeV

    @tf.function
    def call(self, inputs):

        input_rvec = []
        part = inputs.shape[1]
        mr = 0
        sum_m = 0
        risac = 0
        reshape000 = tf.zeros((1,1,1))

        for i in range(part):
          inputsi = tf.split(inputs, part, axis=(1))[i]
          rvec = tf.reduce_sum(inputsi, axis=(1))
          mr = mr + (m[i]*self.MeV * rvec)
          sum_m = sum_m + m[i]*self.MeV

        Rvec_cmw = (mr/sum_m)
        Rvec2_cm = tf.reduce_sum(Rvec_cmw**2, axis=(1))

        for i in range(part):
          inputsi = tf.split(inputs, part, axis=(1))[i]
          rvec = tf.reduce_sum(inputsi, axis=(1))
          delr = rvec - Rvec_cmw

          inputs_rvec = delr + reshape000
          inputs_rvec = tf.reshape(inputs_rvec ,(500,1,-1))
          input_rvec.append(inputs_rvec)

          delrbd = tf.sqrt(tf.reduce_sum(delr**2, axis=(1)))
          risac = risac + delrbd

        Rboundary2_condition = -tf.exp(self.containment1) * Rvec2_cm
        Rboundary2_condition = tf.reshape(Rboundary2_condition, [-1,1])

        rRboundary_condition = -tf.exp(self.containment2) * risac
        rRboundary_condition = tf.reshape(rRboundary_condition, [-1,1])



        input_rvec = tf.concat(input_rvec, axis=(1))
        input = tf.reshape(input_rvec, [500, -1])

        x = self.layer1(input)
        x = tf.keras.activations.softplus(x)

        x = self.layer2(x)
        x = tf.keras.activations.softplus(x)

        x = self.layer3(x)
        #x = tf.keras.activations.tanh(x)

        return x + rRboundary_condition + Rboundary2_condition

ground_state = NeuralWavefunction(3, initial_containment1 = 0.5, initial_containment2 = 0.5)

"""## Metropolis Sampling

To sample the wave function, we'll use the metropolis algorithm.  We can implement this algorithm in tensorflow as shown below:
"""

class MetropolisSampler(object):
    def __init__(self,
        n           : int,
        npart       : int,
        nwalkers    : int,
        initializer : callable,
        init_params : iter ,
        dtype       = tf.float64):

        # dimension:
        self.n = n

        # particle:
        self.npart = npart

        # walkers number:
        self.nwalkers = nwalkers

        self.size = (self.nwalkers, self.npart, self.n)

        self.dtype = dtype

        self.walkers = initializer(shape=self.size, **init_params, dtype=dtype)


    def part(self):
        return  self.npart

    def sample(self):
        return  self.walkers

    def kick(self,
        wavefunction : tf.keras.models.Model,
        kicker : callable,
        kicker_params : iter,
        nkicks : int ):

        walkers, acceptance = self.internal_kicker(self.size, self.walkers, wavefunction, kicker, kicker_params, tf.constant(nkicks), dtype=self.dtype)
        self.walkers = walkers

        return acceptance

    @tf.function
    def internal_kicker(self,
        _shape,
        _walkers,
        _wavefunction : tf.keras.models.Model,
        _kicker : callable,
        _kicker_params : iter,
        _nkicks : tf.constant,
        dtype):

        acceptance = tf.convert_to_tensor(0.0, dtype=dtype)

        current_wavefunction = _wavefunction(_walkers)

        random_numbers = tf.math.log(tf.random.uniform(shape = [_nkicks, _shape[0], 1], dtype=dtype))

        kicks = _kicker(shape=[_nkicks, *_shape], **_kicker_params, dtype=dtype)

        for i_kick in tf.range(_nkicks):
            kick = kicks[i_kick]
            kicked = _walkers + kick

            kicked_wavefunction   = _wavefunction(kicked)
            probability = 2 * (kicked_wavefunction - current_wavefunction)
            accept      = probability >  random_numbers[i_kick]
            current_wavefunction = tf.where(accept, kicked_wavefunction, current_wavefunction)

            accept = tf.tile(accept, [1,tf.reduce_prod(_shape[1:])])
            accept = tf.reshape(accept, _shape)
            _walkers = tf.where(accept, kicked, _walkers)
            acceptance = tf.reduce_mean(tf.cast(accept, dtype))

        return _walkers, acceptance

"""Now that we have a surrogate model and a sampling tool, we can test things:"""

sampler = MetropolisSampler(
            n           = 3, # 3 dimensions
            npart       = 4,
            nwalkers    = N_WALKERS,
            initializer = tf.random.normal,
            init_params = {"mean": 0.0, "stddev" : 0.3},
            dtype       = DEFAULT_TENSOR_TYPE
    )

ground_x = sampler.sample()

"""If we look at X, we should see a 100 x 3 tensor.  Elements from the tensor should roughly follow a gaussian distribution since that's what we used to initialize it (mean 0, STD 0.2)"""

print(ground_x.shape)
print(tf.reduce_mean(ground_x))
print(tf.math.reduce_std(ground_x))

"""Now we can print a summary of the network, too:"""

values = ground_state(ground_x)
ground_state.summary()
print(values.shape)

"""Note: the sum of all printed parameters is one short than the total number of trainable parameters.  That's because the confinement is also a trainable parameter!

## Thermalize the walkers

To thermalize the walkers, we can "kick" them a number of times according to the metropolis algorithm:
"""

view_ri = 0
ground_x = sampler.sample()
wolker_npart = ground_x.shape[1]
print("particle =", wolker_npart)
for i in range(wolker_npart):
  view_wolkeri = tf.split(ground_x, wolker_npart, axis=(1))[i]
  view_wolkeri = tf.reduce_sum(view_wolkeri, axis=(1))
  r_i_thermalization = tf.sqrt(tf.reduce_sum(view_wolkeri**2, axis=1))
  view_ri = view_ri + r_i_thermalization

acceptance = sampler.kick(
    wavefunction  = ground_state ,
    kicker        = tf.random.normal,
    kicker_params = {"mean": 0.0, "stddev" : 0.3},
    nkicks        = 5000)

view_rf = 0
ground_x = sampler.sample()
wolker_npart = ground_x.shape[1]
for i in range(wolker_npart):
  view_wolkeri = tf.split(ground_x, wolker_npart, axis=(1))[i]
  view_wolkeri = tf.reduce_sum(view_wolkeri, axis=(1))
  r_f_thermalization = tf.sqrt(tf.reduce_sum(view_wolkeri**2, axis=1))
  view_rf = view_rf + r_f_thermalization
print("Acceptance: ", acceptance)

view_ri_mean = view_ri/wolker_npart
view_rf_mean = view_rf/wolker_npart

bins = numpy.arange(0,10,0.5)

print(f"Mean r before thermalization: {tf.reduce_mean(view_ri_mean):.4f}")
print(f"Mean r after thermalization: {tf.reduce_mean(view_rf_mean):.4f}")

before_hist, edges = numpy.histogram(view_ri_mean, bins=bins)
after_hist,  edges = numpy.histogram(view_rf_mean, bins=bins)

bin_centers = 0.5*(edges[1:] + edges[:-1])


fig = plt.figure(figsize=(16,9))
plt.plot(bin_centers, before_hist, marker="+", label="Before Therm.")
plt.plot(bin_centers, after_hist, marker="o", label="After Therm.")
plt.grid(True)
plt.legend()
plt.show()

print(acceptance)

"""## Hydrogen Atom Hamiltonian

Here's an implementation of the hydrogen atom that computes observables:
"""

class HydrogenAtom(object):

    def __init__(self, sampler):

        object.__init__(self)
        # Natural Unit constant
        self.fm = tf.constant(fm, dtype = DEFAULT_TENSOR_TYPE2)
        self.fm2 = self.fm**2
        self.ifm = tf.constant(fm_inverse, dtype = DEFAULT_TENSOR_TYPE2)
        self.ifm2 = self.ifm**2
        self.epsilon0 = tf.constant(epsilon0, dtype = DEFAULT_TENSOR_TYPE2)
        self.mass = m
        self.MeV = tf.constant(MeV, dtype = DEFAULT_TENSOR_TYPE2)
        self.hbar = tf.constant(hbar, dtype = DEFAULT_TENSOR_TYPE2)
        self.c = tf.constant(c, dtype = DEFAULT_TENSOR_TYPE2)
        self.hbarc = tf.constant(hbarc, dtype = DEFAULT_TENSOR_TYPE2)
        self.alpha = tf.constant(alpha, dtype = DEFAULT_TENSOR_TYPE2)

        # Harmonic Oscillator constant
        self.hbaromega = tf.constant(32.0 , dtype = DEFAULT_TENSOR_TYPE2)*self.MeV
        self.harmonic_constant = tf.constant(3.0/2.0, dtype = DEFAULT_TENSOR_TYPE2)

        # Particle n
        self.npart = sampler.part()

        # Potential constant
        self.Gamma = tf.constant(-1.0*self.ifm2, dtype = DEFAULT_TENSOR_TYPE2)
        self.D0 = tf.constant(0.068*GeV, dtype = DEFAULT_TENSOR_TYPE2)
        self.C10 = tf.constant(-0.142*GeV, dtype = DEFAULT_TENSOR_TYPE2)
        self.C01 = tf.constant(-0.106*GeV, dtype = DEFAULT_TENSOR_TYPE2)
        self.sigma_spine = tf.constant(-3.0, dtype = DEFAULT_TENSOR_TYPE2)
        self.sigma_spine2 = tf.constant(0.0, dtype = DEFAULT_TENSOR_TYPE2)

    @tf.function
    def energy(self, wavefunction : tf.keras.models.Model, inputs : tf.Tensor):

        inputs = tf.cast(inputs, DEFAULT_TENSOR_TYPE2)
        logw_of_x, dlogw_dx, d2logw_dx2 = self.compute_derivatives(wavefunction, inputs)

        ke_direct, interaction_e = self.compute_energies(inputs, logw_of_x, dlogw_dx, d2logw_dx2)
        energy = ke_direct + interaction_e
        energy = energy - (self.harmonic_constant * self.hbaromega)
        energy = tf.squeeze(energy/self.MeV)
        return tf.cast(energy, DEFAULT_TENSOR_TYPE)

    @tf.function
    def compute_derivatives(self, wavefunction : tf.keras.models.Model, inputs : tf.Tensor):

        with tf.GradientTape(persistent=True) as tape:
            tape.watch(inputs)
            with tf.GradientTape() as second_tape:
                second_tape.watch(inputs)
                logw_of_x = wavefunction(inputs, training=True)

            dlogw_dx = second_tape.gradient(logw_of_x, inputs)
        d2logw_dx2 = tape.batch_jacobian(dlogw_dx, inputs)

        d2logw_dx2 = tf.einsum("wvdvd->wvd",d2logw_dx2)
        #d2logw_dx2= tf.reshape(d2logw_dx2, [500, -1])
        #dlogw_dx= tf.reshape(dlogw_dx, [500, -1])

        return logw_of_x, dlogw_dx, d2logw_dx2

    @tf.function
    def interaction_energy(self, *, logw_of_x, inputs):
        interaction1 = 0
        interaction2 = 0
        sum_mr = 0
        M_total = 0

        if self.npart > 1:
          # Two body interaction
          for i in range(self.npart-1):

            walkero = tf.split(inputs, self.npart, axis=(1))[i]

            for j in range(i+1, self.npart):

              walkerm = tf.split(inputs, self.npart, axis=(1))[j]
              dwalker = walkero - walkerm
              walker_dnm = tf.reduce_sum(dwalker, axis=(1))
              del_r = tf.sqrt(tf.reduce_sum(walker_dnm**2, axis=(1)))*self.fm
              del_r_2 = tf.reduce_sum(walker_dnm**2, axis=(1))

              if i==0 and j==1:
                interaction1o = (1.0/4.0*((3.0*self.C10) + self.C01 + (self.C10 - self.C01)*self.sigma_spine))*tf.exp(self.Gamma*del_r_2)
                interaction1 = interaction1 + interaction1o

              elif i==2 and j==3:
                interaction1o = (1.0/4.0*((3.0*self.C10) + self.C01 + (self.C10 - self.C01)*self.sigma_spine))*tf.exp(self.Gamma*del_r_2)
                pinteraction = self.alpha*self.hbarc*(1/(del_r+1e-8))
                interaction1 = interaction1 + interaction1o + pinteraction

              else:
                interaction1o = (1.0/4.0*((3.0*self.C10) + self.C01 + (self.C10 - self.C01)*self.sigma_spine2))*tf.exp(self.Gamma*del_r_2)
                interaction1 = interaction1 + interaction1o


          # Three Body Interaction
          if self.npart > 2:
            for i in range(self.npart-2):
              for j in range(i+1, self.npart-1):
                for k in range(j+1, self.npart):

                  rij = tf.reduce_sum((tf.split(inputs, self.npart, axis=(1))[i] - tf.split(inputs, self.npart, axis=(1))[j]), axis=(1))*self.fm
                  rik = tf.reduce_sum((tf.split(inputs, self.npart, axis=(1))[i] - tf.split(inputs, self.npart, axis=(1))[k]), axis=(1))*self.fm
                  rjk = tf.reduce_sum((tf.split(inputs, self.npart, axis=(1))[j] - tf.split(inputs, self.npart, axis=(1))[k]), axis=(1))*self.fm

                  rij_2 = tf.reduce_sum(rij**2, axis=(1))
                  rik_2 = tf.reduce_sum(rik**2, axis=(1))
                  rjk_2 = tf.reduce_sum(rjk**2, axis=(1))

                  ir1 =  rij_2 + rik_2
                  ir2 =  rij_2 + rjk_2
                  ir3 =  rik_2 + rjk_2

                  interaction21 = self.D0 * tf.exp(self.Gamma*ir1)
                  interaction22 = self.D0 * tf.exp(self.Gamma*ir2)
                  interaction23 = self.D0 * tf.exp(self.Gamma*ir3)
                  interaction2 = (interaction21 + interaction22 + interaction23) + interaction2
          else:
            interaction2 = 0

          # Harmonic Oscillator
          for i in range(self.npart):
            hwalker = tf.split(inputs, self.npart, axis=(1))[i]
            hwalker = tf.reduce_sum(hwalker, axis=(1))
            mo = tf.cast((self.mass[i]*self.MeV), dtype= DEFAULT_TENSOR_TYPE2)
            sum_mr = sum_mr + (mo * hwalker)*self.fm
            M_total = M_total + mo
          sum_R = sum_mr / M_total
          sum_R2 = tf.reduce_sum(sum_R**2 , axis =(1))

          k = M_total* (self.hbaromega**2) / (self.hbarc**2)
          harmonic_pot = tf.cast(0.5, dtype= DEFAULT_TENSOR_TYPE2) * k * sum_R2
          interaction = interaction1 + interaction2 + harmonic_pot
        else:
          interaction = 0

        return interaction

    @tf.function
    def tot_kinetic_energy(self, *, logw_of_x: tf.Tensor, dlogw_dx: tf.Tensor, d2logw_dx2 : tf.Tensor):

        ke = 0

        for i in range(self.npart):

          dlogw_dxi = tf.split(dlogw_dx, self.npart, axis = (1))[i]
          dlogw_dxi = tf.reduce_sum(dlogw_dxi, axis=(1))*self.fm
          d2logw_dx2i = tf.split(d2logw_dx2, self.npart, axis = (1))[i]
          d2logw_dx2i = tf.reduce_sum(d2logw_dx2i, axis=(1))*self.fm2

          ke_i = -(self.hbarc**2 / (2 * tf.cast((self.mass[i]*self.MeV), DEFAULT_TENSOR_TYPE2))) * (tf.reduce_sum(d2logw_dx2i, axis=(1)) + tf.reduce_sum(dlogw_dxi**2, axis=(1)))
          ke = ke + ke_i

        return ke


    @tf.function
    def compute_energies(self, inputs, logw_of_x, dlogw_dx, d2logw_dx2):

        ke_direct = self.tot_kinetic_energy(logw_of_x = logw_of_x,dlogw_dx=dlogw_dx, d2logw_dx2 = d2logw_dx2)

        interaction = self.interaction_energy(logw_of_x=logw_of_x, inputs=inputs)

        return ke_direct, interaction

hamiltonian = HydrogenAtom(sampler)

energy = hamiltonian.energy(ground_state, ground_x)

print(tf.reduce_mean(energy))

"""In the units we're using (atomic units) the value of the hydrogen ground state comes out to -0.5.  So, this is good: the energy is above the ground state for our random wavefunction which is good!

## Optimizing the wavefunction

To optimize the wavefunction, we'll need to compute several observables based on the jacobian.  Here are some functions to help contained within a `compute_gradients` function.
"""

def compute_gradients(N_OBSERVATIONS, _sampler, _wavefunction, _hamiltonian):

    @tf.function
    def jacobian(j_x_current, j_wavefunction, _sampler):
        tape = tf.GradientTape()
        with tape:
          tape.watch(j_wavefunction.trainable_variables)
          log_wpsi = j_wavefunction(j_x_current)

        jac = tape.jacobian(log_wpsi, j_wavefunction.trainable_variables)

        jac_shape = [j.shape[1:] for j in jac]
        flat_shape = [[-1, tf.reduce_prod(js)] for js in jac_shape]
        flattened_jacobian = [tf.reshape(j, f) for j, f in zip(jac, flat_shape)]
        flattened_jacobian = tf.concat(flattened_jacobian, axis=-1)

        return flattened_jacobian, flat_shape

    @tf.function
    def compute_O_observables(flattened_jacobian, energy):
        dpsi_i = tf.reduce_mean(flattened_jacobian, axis=0)
        dpsi_i = tf.reshape(dpsi_i, [-1,1])

        dpsi_ij = tf.linalg.matmul(flattened_jacobian, flattened_jacobian, transpose_a = True) / N_WALKERS

        dpsi_i_EL = tf.linalg.matmul(tf.reshape(energy, [1,N_WALKERS]), flattened_jacobian)
        dpsi_i_EL = tf.reshape(dpsi_i_EL, [-1, 1])

        return dpsi_i, dpsi_i_EL, dpsi_ij

    def compute_SR_gradients(crs_energy, csr_dpsi_i, csr_dpsi_i_EL, csr_dpsi_ij):

        eps = 0.001
        npt = csr_dpsi_i.shape[0]

        gradients = tf.cast(( csr_dpsi_i * crs_energy - csr_dpsi_i_EL ), tf.float64)

        S_ij = tf.cast(csr_dpsi_ij - csr_dpsi_i * tf.transpose(csr_dpsi_i), tf.float64)

        i = 0
        while True:
            S_ij_d = S_ij + 2**i * eps * tf.eye(npt, dtype=tf.float64)
            i += 1
            try:
                U_ij = tf.linalg.cholesky(S_ij_d)
                positive_definite = True
            except:
                positive_definite = False
                print(f"Cholesky solve did not find a positive definite matrix on attempt {i}!")
            if positive_definite:
                gradients = tf.linalg.cholesky_solve(U_ij, gradients)
                break

        return tf.cast(gradients, tf.float32)




    dpsi_i    = None
    dpsi_i_EL = None
    dpsi_ij   = None
    obs_energy = 0

    for i_obs in range(N_OBSERVATIONS):
        kicker = tf.random.normal
        kicker_params = {"mean": 0.0, "stddev" : 0.4}
        acceptance = _sampler.kick(_wavefunction, kicker, kicker_params, nkicks=100)
        x_current  = _sampler.sample()
        energy = hamiltonian.energy(_wavefunction, x_current)
        energy /= N_WALKERS

        flattened_jacobian, flat_shape = jacobian(x_current, _wavefunction, _sampler)

        _dpsi_i, _dpsi_i_EL, _dpsi_ij = compute_O_observables(flattened_jacobian, energy)
        if dpsi_i is None:
            dpsi_i = _dpsi_i
        else:
            dpsi_i += _dpsi_i

        if dpsi_i_EL is None:
            dpsi_i_EL = _dpsi_i_EL
        else:
            dpsi_i_EL += _dpsi_i_EL

        if dpsi_ij is None:
            dpsi_ij = _dpsi_ij
        else:
            dpsi_ij += _dpsi_ij

        obs_energy += tf.reduce_sum(energy)

    obs_energy /= N_OBSERVATIONS
    dpsi_i     /= N_OBSERVATIONS
    dpsi_i_EL  /= N_OBSERVATIONS
    dpsi_ij    /= N_OBSERVATIONS


    gradients = compute_SR_gradients(obs_energy, dpsi_i, dpsi_i_EL, dpsi_ij)


    # Lastly, reshape the gradients to match the weights:
    running_index = 0
    gradient = []
    for length in flat_shape:
        l = length[-1]
        end_index = running_index + l
        gradient.append(gradients[running_index:end_index])
        running_index += l

    shapes = [ p.shape for p in _wavefunction.trainable_variables ]
    gradients = [ tf.reshape(g, s) for g, s in zip(gradient, shapes)]
    return gradients, obs_energy

# optimizer = tf.keras.optimizers.Adam()
energy_history = []
steps_history  = []
LEARNING_RATE  = 0.001

"""## Optimization Loop"""

# Define a figure here which we can update along the way:
fig       = plt.figure(figsize=(16,9))
# Create 2x2 sub plots
gs = gridspec.GridSpec(2, 2)
ax_loss   = plt.subplot(gs[:,0])
ax_denst  = plt.subplot(gs[:,1], projection="3d")

for i in range(4000):

    gradients, energy = compute_gradients(N_OBSERVATIONS, sampler, ground_state, hamiltonian)
    # Scale by the learning rate
    gradients = [LEARNING_RATE * g for g in gradients]

    for i_param in range(len(ground_state.trainable_variables)):
        ground_state.trainable_variables[i_param].assign_add(gradients[i_param])
    print(tf.reduce_mean(energy))

    energy_history.append(energy)
    steps_history.append(i)
    if energy < -18 and LEARNING_RATE == 0.001:
      LEARNING_RATE  = 0.0001
    print(LEARNING_RATE)
    # Here we update the plots.  Show the energy function:
    ax_loss.set_xlim(0, 1.1*i)
    ax_loss.cla()
    ax_loss.plot(steps_history, energy_history, label="Energy")
    ax_loss.grid(True)
    ax_loss.legend(fontsize=25)
    ax_loss.set_ylabel('ev')

    # Show the location of the walkers:


    wolker_npart = ground_x.shape[1] #파티클 수
    for i in range(wolker_npart):
      view_wolkeri = tf.split(ground_x, wolker_npart, axis=(1))[i]
      view_wolkeri = tf.reduce_sum(view_wolkeri, axis=(1))
      x, y, z = tf.split(view_wolkeri, 3, axis=1)
      ax_denst.clear()
      ax_denst.set_xlim(-4,4); ax_denst.set_ylim(-4,4); ax_denst.set_zlim(-4,4);
      ax_denst.set_xlabel("X"); ax_denst.set_ylabel("Y"); ax_denst.set_zlabel("Z");
      ax_denst.scatter(x, y, z, linewidth=0.5);

    display(fig)
    clear_output(wait = True)
#     plt.pause(0.25)

"""2체 potential에 sigma 부분에 두개는 0 1개는 -3으로해서 하는 계산"""

plt.savefig("./energy-coll.png")

"""2체 potential에 sigma 부분에 두개는 0 1개는 -3으로해서 하는 계산"""

output = open('./LAB_frame_He3_output.data','w+')
for i in range(len(energy_history)):
  output.write('%12.8f,%12.8f \n'%( i, energy_history[i]))
output.close()



"""This can take a while to converge.  If you are running on, say, a CPU or just don't want to wait, you can access my pretrained model:"""

# Run this if you cloned the repo locally:
# ground_state = tf.keras.models.load_model("saved_models/HydrogenGroundState")

# Run this if you are on colab:
# !git clone https://github.com/coreyjadams/AI4NP_School.git
# ground_state = tf.keras.models.load_model("AI4NP_School/saved_models/HydrogenGroundState")

"""This checks the final energy computed:"""

kicker = tf.random.normal
kicker_params = {"mean": 0.0, "stddev" : 0.4}

acceptance = sampler.kick(ground_state, kicker, kicker_params, nkicks=5000)
x = sampler.sample()
energy = hamiltonian.energy(ground_state, x)
print(tf.reduce_mean(energy))

# print(ground_state.containment) # (If you load my model, this should be 0.011789099)

# 유효한 구간 찾기
plt.figure(figsize=(10,10))
plt.rcParams["figure.figsize"] = (10, 10)
check_std_array = []
check_std = []
check_averages = []
check_interval = []
check_average = 0
check_section = 20
for i in range(int(len(energy_history)/check_section)):
  ia = i+1
  for j in range(check_section*i, check_section*ia):
    check_average = check_average + energy_history[j]
    check_std_array.append(energy_history[j])

  check_interval.append(check_section*(ia))
  check_average = check_average/check_section
  check_averages.append(check_average)
  check_std.append(numpy.std(check_std_array))
  check_std_array = []
  check_average = 0

print(len(check_interval), len(check_averages), len(check_std))

check_x = check_interval
check_y = check_averages
check_e = check_std

output = open('./LAB_frame_He3_check.data','w+')
for i in range(len(check_y)):
  output.write('%12.8f,%12.8f,%12.8f \n'%(check_x[i], check_y[i], check_std[i]))
output.close()

plt.errorbar(check_x, check_y, check_e, linestyle='None', marker='', label="Energy")
plt.grid(True)
plt.savefig("./before.png")
#plt.show()
# 확대
plt.errorbar(check_x, check_y, check_e, linestyle='None', marker='', label="Energy")

plt.grid(True)
plt.savefig("./after.png")
#plt.show()

# 유효한 구간 확대
choose_x = steps_history
choose_y = energy_history
plt.ylim(check_averages[-1]-check_std[-1]-3, check_averages[-3]+check_std[-3]+3) # 위의 제일 뒤에서 check_section만큼 떨어진 구간별 표춘편차로 y축 확대 위치 지정
plt.xlim(numpy.array(check_interval[-7]), numpy.array(check_interval[-1])) # 위의 제일 뒤에서 check_section으로 잘라 x축 확대 위치 지정
plt.plot(choose_x, choose_y)
#plt.rcParams["figure.figsize"] = (1, 4)
plt.grid(True)
plt.savefig("./effective-range.png")
#plt.show()

average = 0
average_array = []
for i in range(numpy.array(check_interval[-7]), numpy.array(check_interval[-1])):
  average = average + energy_history[i]
  average_array.append(energy_history[i])
chose_section = numpy.array(check_interval[-1]) - numpy.array(check_interval[-7])

average = average / int(chose_section)
std = numpy.std(average_array)
print(average, std)

# Want to save your model?  Use this syntax
# tf.keras.models.save_model(ground_state, "HydrogenGroundState")

"""This can take a while to converge.  If you are running on, say, a CPU or just don't want to wait, you can access my pretrained model:"""

# Run this if you cloned the repo locally:
# ground_state = tf.keras.models.load_model("saved_models/HydrogenGroundState")

# Run this if you are on colab:
# !git clone https://github.com/coreyjadams/AI4NP_School.git
# ground_state = tf.keras.models.load_model("AI4NP_School/saved_models/HydrogenGroundState")

"""This checks the final energy computed:"""

kicker = tf.random.normal
kicker_params = {"mean": 0.0, "stddev" : 0.4}

acceptance = sampler.kick(ground_state, kicker, kicker_params, nkicks=5000)
x = sampler.sample()
energy = hamiltonian.energy(ground_state, x)
print(tf.reduce_mean(energy))

# print(ground_state.containment) # (If you load my model, this should be 0.011789099)

# 유효한 구간 찾기
plt.figure(figsize=(10,10))
plt.rcParams["figure.figsize"] = (10, 10)
check_std_array = []
check_std = []
check_averages = []
check_interval = []
check_average = 0
check_section = 20
for i in range(int(len(energy_history)/check_section)):
  ia = i+1
  for j in range(check_section*i, check_section*ia):
    check_average = check_average + energy_history[j]
    check_std_array.append(energy_history[j])

  check_interval.append(check_section*(ia))
  check_average = check_average/check_section
  check_averages.append(check_average)
  check_std.append(numpy.std(check_std_array))
  check_std_array = []
  check_average = 0

print(len(check_interval), len(check_averages), len(check_std))

check_x = check_interval
check_y = check_averages
check_e = check_std

plt.errorbar(check_x, check_y, check_e, linestyle='None', marker='', label="Energy")
plt.grid(True)
plt.show()
# 확대
plt.errorbar(check_x, check_y, check_e, linestyle='None', marker='', label="Energy")
plt.grid(True)
plt.show()

# 유효한 구간 확대
choose_x = steps_history
choose_y = energy_history
plt.ylim(check_averages[-1]-check_std[-1]-1, check_averages[-3]+check_std[-3]+1) # 위의 제일 뒤에서 check_section만큼 떨어진 구간별 표춘편차로 y축 확대 위치 지정
plt.xlim(numpy.array(check_interval[-7]), numpy.array(check_interval[-1])) # 위의 제일 뒤에서 check_section으로 잘라 x축 확대 위치 지정
plt.plot(choose_x, choose_y)
#plt.rcParams["figure.figsize"] = (1, 4)
plt.grid(True)
plt.show()

average = 0
average_array = []
for i in range(numpy.array(check_interval[-7]), numpy.array(check_interval[-1])):
  average = average + energy_history[i]
  average_array.append(energy_history[i])
chose_section = numpy.array(check_interval[-1]) - numpy.array(check_interval[-7])

average = average / int(chose_section)
std = numpy.std(average_array)
print(average, std)

"""### Where are the walkers?

Now that we've trained the wavefunction, where are the walkers?
"""

r_ground  = tf.sqrt(tf.reduce_sum(ground_x**2, axis=1))

print(f"Mean r for the ground state: {tf.reduce_mean(r_ground):.4f}")

bins = numpy.arange(0,15,0.5)

ground_hist,  edges = numpy.histogram(r_ground, bins=bins)

bin_centers = 0.5*(edges[1:] + edges[:-1])


fig = plt.figure(figsize=(16,9))
plt.scatter(bin_centers, ground_hist, marker="+", label="Ground State")
plt.grid(True)
plt.legend()
plt.show()

"""## Computing Observables

If you look online ([here](http://farside.ph.utexas.edu/teaching/qmech/Quantum/node82.html) for example) there are many resources that tell you about the hydrogen atom's observable values.  $<r>$ for example should be equal to $\frac{a_0}{2} [ 3 n^2 - l(l+1)]$.  We don't have any angular momentum in the ground state, and the value of $a_0$ in our units is 1.0.  So, $<r>$ should be 1.5
"""

r_accum = 0.0
r2_accum = 0.0

kicker = tf.random.normal
kicker_params = {"mean": 0.0, "stddev" : 0.4}

for i in range(N_OBSERVATIONS):
    acceptance = sampler.kick(ground_state, kicker, kicker_params, nkicks=200)
    ground_x = sampler.sample()

    r2 = tf.reduce_sum(ground_x**2, axis=1)
    r2_accum += tf.reduce_sum(r2)
    r_ground  = tf.sqrt(r2)
    r_accum  += tf.reduce_sum(r_ground)


n_walkers = ground_x.shape[0]
r_accum /= N_OBSERVATIONS*n_walkers
r_err = 1/(N_OBSERVATIONS*n_walkers - 1) * tf.sqrt(r2_accum - r_accum**2)
print(f"Expectation of radius is: {r_accum:.3f} +/ {r_err:.3f}")

"""On the other side, $<1/r>$ should be equal to 1.0 in our units:"""

r_accum = 0.0
r2_accum = 0.0

kicker = tf.random.normal
kicker_params = {"mean": 0.0, "stddev" : 0.4}


for i in range(N_OBSERVATIONS):
    acceptance = sampler.kick(ground_state, kicker, kicker_params, nkicks=200)
    ground_x = sampler.sample()

    r2 = 1. / tf.reduce_sum(ground_x**2, axis=1)
    r2_accum += tf.reduce_sum(r2)

    r_ground  = tf.sqrt(r2)
    r_accum += tf.reduce_sum(r_ground)

n_walkers = ground_x.shape[0]
r_accum /= N_OBSERVATIONS*n_walkers
r_err = 1/(N_OBSERVATIONS*n_walkers - 1) * tf.sqrt(r2_accum - r_accum**2)

print(f"Expectation of 1/radius is: {r_accum:.3f} +/- {r_err:.3f}")

"""## Angular momentum

The angular momentum operator $L_z$ has good quantum numbers, so let's compute it's expectation value for the ground state.  Remember, here, we parametrize the log of the wavefunction ($log(\psi)$).  Note that:

### $\frac{\partial}{\partial x} log(\psi) == \frac{1}{\psi} \frac{\partial \psi}{\partial x}$

and

### $L_z = -i \hbar (x \frac{\partial \psi}{\partial y} - y \frac{\partial \psi}{\partial x}) $

To compute the observable, we sum:

### $<L_z> = \sum \frac{L_z}{\psi}  = -i \hbar \sum \left( x \frac{1}{\psi} \frac{\partial \psi}{\partial y} -  y  \frac{1}{\psi} \frac{\partial \psi}{\partial x} \right)$

or, using the first equation above:

### $<L_z> = -i \hbar \sum \left( x \frac{\partial}{\partial y} log(\psi) - y \frac{\partial}{\partial x} log(\psi) \right) $

We can compute the z component of angular momentum, then, in units of $-i \hbar$:
"""

lz_accum = 0.0
lz2_accum = 0.0

l2_accum = 0.0
l22_accum = 0.0

kicker = tf.random.normal
kicker_params = {"mean": 0.0, "stddev" : 0.4}

for i in range(N_OBSERVATIONS):

    acceptance = sampler.kick(ground_state, kicker, kicker_params, nkicks=200)
    ground_x = sampler.sample()


    logw_of_x, dlogw_dx, d2logw_dx2 = hamiltonian.compute_derivatives(ground_state,ground_x)

#     print(ground_x.shape)

    lx = ground_x[:,1]*dlogw_dx[:,2] - ground_x[:,2] * dlogw_dx[:,1]
    ly = ground_x[:,2]*dlogw_dx[:,0] - ground_x[:,0] * dlogw_dx[:,2]
    lz = ground_x[:,0]*dlogw_dx[:,1] - ground_x[:,1] * dlogw_dx[:,0]

    lz_accum  += tf.reduce_sum(lz)
    lz2_accum += tf.reduce_sum(lz**2)


    l2 = tf.sqrt(lx**2 + ly**2 + lz**2)
    l2_accum += tf.reduce_sum(l2)
    l22_accum += tf.reduce_sum(l2**2)
n_walkers = ground_x.shape[0]

lz_accum /= N_OBSERVATIONS*n_walkers
lz_err = 1/(N_OBSERVATIONS*n_walkers - 1) * tf.sqrt(lz2_accum - lz_accum**2)

print(f"Expectation of L_z is: -i * hbar * {lz_accum:.4f} +/- {lz_err:.4f}")

l2_accum /= N_OBSERVATIONS*n_walkers
l2_err = 1/(N_OBSERVATIONS*n_walkers - 1) * tf.sqrt(l22_accum - l2_accum**2)

print(f"Expectation of L^2 is: -i * hbar * {l2_accum:.4f} +/- {l2_err:.4f}")

